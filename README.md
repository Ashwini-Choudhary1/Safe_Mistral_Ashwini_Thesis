# ğŸ›¡ï¸ Thesis Safety Checker

A Streamlit-based application that detects unsafe content in prompts and responses using a quantized Mistral-7B language model. It includes safeguards against harmful, restricted, or competitive content and provides structured, user-friendly output.

---



-  Inference with quantized Mistral-7B-Instruct
- Custom content moderation via keyword and topic filters (designed for autistic people)
- Input & output scanning for harmful or policy-violating content
- Structured responses with summaries, examples, and clarity
- Lightning AI deployment-ready

---

## ğŸ“¦ Installation
